import streamlit as st
import pandas as pd
from agent.supply_chain_agent import SupplyChainAgent
from agent.regional_agent import RegionalAgent
from agent.industry_agent import IndustryAgent  # ç¡®ä¿è·¯å¾„æ­£ç¡®
from agent.financial_agent import FinancialAgent
import tempfile
import os

st.set_page_config(page_title="è¡Œä¸šæ™ºèƒ½åˆ†æ", layout="wide")

st.title("è¡Œä¸šæ™ºèƒ½åˆ†æåŠ©æ‰‹")

# Tabåˆ‡æ¢ï¼šè¡Œä¸šåˆ†æ/åœ°åŒºåˆ†æ
#tabs = st.tabs(["ä¾›åº”é“¾åˆ†æ", "åœ°åŒºåˆ†æ"])
tabs = st.tabs(["ä¾›åº”é“¾åˆ†æ", "åœ°åŒºåˆ†æ", "è¡Œä¸šæŠ¥å‘Šåˆ†æ","è´¢åŠ¡æŠ¥è¡¨åˆ†æ"])

from pathlib import Path

# å½“å‰è„šæœ¬è·¯å¾„
current_file = Path(__file__).resolve()

# é¡¹ç›®æ ¹ç›®å½•ï¼Œå‡è®¾ Q-MacroAgent ä¸ tavily-company-research åŒçº§
project_root = current_file.parents[1]  # Q-MacroAgent/ä¸Šä¸€çº§ -> projects/

# ========== ä¾›åº”é“¾åˆ†æ Tab ==========
with tabs[0]:
    st.write("è¯·ä¸Šä¼ åŒ…å«ä¼ä¸šå› å­å’Œå…³è”æƒé‡çš„CSVæ–‡ä»¶ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨è¿›è¡Œä¾›åº”é“¾&ä¸Šä¸‹æ¸¸è¶‹åŠ¿åˆ†æå’Œé¢„æµ‹ã€‚")
    uploaded_file = st.file_uploader("ä¸Šä¼ CSVæ–‡ä»¶", type=["csv"], key="industry_csv")
    if uploaded_file is not None:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".csv") as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_path = tmp_file.name
        df = pd.read_csv(tmp_path)
        st.subheader("æ•°æ®é¢„è§ˆ")
        st.dataframe(df)
        st.subheader("ä¾›åº”é“¾åˆ†ææŠ¥å‘Š")
        with st.spinner("æ­£åœ¨åˆ†æï¼Œè¯·ç¨å€™..."):
            agent = SupplyChainAgent()
            report, graph_img_path = agent.analyze(tmp_path)
        st.success("åˆ†æå®Œæˆï¼")
        st.markdown(f"```\n{report}\n```")
        # å¯é€‰ï¼šå±•ç¤ºè¡Œä¸šå…³ç³»å›¾
        # st.subheader("ä¼ä¸šå…³ç³»å›¾å¯è§†åŒ–")
        # st.image(graph_img_path, caption="è¡Œä¸šä¼ä¸šå…³ç³»å›¾")
    else:
        st.info("è¯·å…ˆä¸Šä¼ CSVæ–‡ä»¶ã€‚")

# ========== åœ°åŒºåˆ†æ Tab ==========
# ========== åœ°åŒºåˆ†æ Tab ========== 
with tabs[1]:
    st.write("è¯·ä¸Šä¼ åŒä¸€åœ°åŒºçš„ä¼ä¸šJSONæŠ¥å‘Šæ–‡ä»¶ï¼ˆå¯å¤šé€‰ï¼‰ï¼Œæˆ–ä»çŸ¥è¯†åº“å¯¼å…¥ã€‚ç³»ç»Ÿå°†è‡ªåŠ¨è¿›è¡Œåœ°åŒºçº§ä¼ä¸šç½‘ç»œä¸é›†ç¾¤åˆ†æã€‚")

    # æ–°å¢é€‰é¡¹ï¼šæ˜¯å¦ä» knowledge_base å¯¼å…¥
    use_kb = st.checkbox("ä»çŸ¥è¯†åº“å¯¼å…¥ JSON æ–‡ä»¶", value=False)
    #knowledge_base_dir = r"d:\projects\tavily-company-research\knowledge_base\company_reports"
    knowledge_base_dir = project_root / "tavily-company-research" / "knowledge_base" / "company_reports"

    uploaded_jsons = []

    if use_kb:
        if os.path.exists(knowledge_base_dir):
            kb_files = [f for f in os.listdir(knowledge_base_dir) if f.endswith(".json")]

            # å…³é”®è¯ç­›é€‰
            filter_text = st.text_input("æŒ‰å…³é”®è¯ç­›é€‰çŸ¥è¯†åº“æ–‡ä»¶ï¼ˆå¯æ‰¹é‡é€‰ä¸­ï¼‰", value="", key="region_kb_filter")
            if filter_text:
                matched_files = [f for f in kb_files if filter_text in f]
            else:
                matched_files = kb_files

            # å…¨é€‰/å–æ¶ˆå…¨é€‰
            select_all = st.checkbox("å…¨é€‰çŸ¥è¯†åº“æ–‡ä»¶", value=False, key="region_kb_all")
            default_selection = matched_files if select_all else []

            selected_files = st.multiselect(
                "é€‰æ‹©çŸ¥è¯†åº“æ–‡ä»¶",
                options=kb_files,
                default=default_selection,
                key="region_kb_files"
            )

            for f in selected_files:
                file_path = os.path.join(knowledge_base_dir, f)
                class FileLike:
                    def __init__(self, path):
                        self.name = os.path.basename(path)
                        self._path = path
                    def getvalue(self):
                        with open(self._path, "rb") as _f:
                            return _f.read()
                uploaded_jsons.append(FileLike(file_path))
        else:
            st.warning(f"çŸ¥è¯†åº“æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {knowledge_base_dir}")
    else:
        uploaded_jsons = st.file_uploader(
            "ä¸Šä¼ ä¼ä¸šJSONæŠ¥å‘Šï¼ˆå¯å¤šé€‰ï¼‰", type=["json"], accept_multiple_files=True, key="region_jsons"
        )

    region_name = st.text_input("è¯·è¾“å…¥åœ°åŒºåç§°ï¼ˆå¦‚ï¼šä¸­å›½ï¼ŒåŒ—äº¬å¸‚ï¼‰", value="ä¸­å›½ï¼ŒåŒ—äº¬å¸‚")

    if uploaded_jsons and region_name:
        # å°†ä¸Šä¼ çš„JSONæ–‡ä»¶ä¿å­˜åˆ°ä¸´æ—¶ç›®å½•
        with tempfile.TemporaryDirectory() as tmp_dir:
            file_paths = []
            for file in uploaded_jsons:
                safe_name = file.name.replace("ï¼Œ","_").replace(" ", "_")
                file_path = os.path.join(tmp_dir, safe_name)
                with open(file_path, "wb") as f:
                    f.write(file.getvalue())
                file_paths.append(file_path)

            # è°ƒè¯•ä¿¡æ¯
            st.write("ä¸´æ—¶ç›®å½•è·¯å¾„:", tmp_dir)
            st.write("æ–‡ä»¶åˆ—è¡¨:", file_paths)

            # è°ƒç”¨RegionalAgentåˆ†æ
            with st.spinner("æ­£åœ¨è¿›è¡Œåœ°åŒºåˆ†æï¼Œè¯·ç¨å€™..."):
                agent = RegionalAgent()
                report = agent.analyze_region(tmp_dir, region_name)

            if report:
                st.success("åˆ†æå®Œæˆï¼")
                st.subheader("åœ°åŒºåˆ†æä¸»è¦ç»“è®º")
                st.markdown(f"**å…¬å¸æ•°é‡ï¼š** {report.get('companies_count', 0)}")
                st.markdown(f"**äº§ä¸šåˆ†å¸ƒï¼š** {report.get('industry_distribution', {})}")
                st.markdown(f"**äº§ä¸šé›†ç¾¤å¼ºåº¦ï¼š** {report.get('regional_insights', {}).get('economic_cluster_strength', {}).get('strength_level', 'æœªçŸ¥')}")
                st.markdown(f"**åˆ›æ–°æ½œåŠ›ï¼š** {report.get('regional_insights', {}).get('innovation_potential', {}).get('potential_level', 'æœªçŸ¥')}")
                st.markdown(f"**ååŒåˆ†æ•°ï¼š** {report.get('network_analysis', {}).get('synergy_score', 'æ— ')}")
                st.markdown(f"**æ¢çº½ä¼ä¸šï¼š** {report.get('network_analysis', {}).get('hub_companies', [])}")
                st.markdown(f"**æŠ•èµ„å»ºè®®ï¼š** {report.get('investment_recommendations', {}).get('recommendation_level', 'æœªçŸ¥')}")

                if 'llm_report' in report:
                    st.subheader("AIæ™ºèƒ½åˆ†ææŠ¥å‘Š")
                    st.markdown(report['llm_report'])

                graph_img = report.get('network_analysis', {}).get('graph_image', None)
                if graph_img and os.path.exists(graph_img):
                    st.subheader("ä¼ä¸šç½‘ç»œå…³ç³»å›¾ï¼ˆç²¾ç®€ç‰ˆï¼‰")
                    st.image(graph_img, caption="åœ°åŒºä¼ä¸šå…³ç³»ç½‘ç»œ")

                txt_path = f"regional_report_{region_name.replace('ï¼Œ','_')}.txt"
                if os.path.exists(txt_path):
                    with open(txt_path, "r", encoding="utf-8") as f:
                        txt_content = f.read()
                    st.subheader("åˆ†ææŠ¥å‘Šæ–‡æœ¬ï¼ˆå¯å¤åˆ¶/ä¸‹è½½ï¼‰")
                    st.text_area("åœ°åŒºåˆ†ææŠ¥å‘Š", txt_content, height=400)
                    st.download_button("ä¸‹è½½txtæŠ¥å‘Š", txt_content, file_name=txt_path)
            else:
                st.error("æœªç”Ÿæˆåœ°åŒºåˆ†ææŠ¥å‘Šï¼Œè¯·æ£€æŸ¥æ•°æ®æ ¼å¼æˆ–å†…å®¹ã€‚")
    else:
        st.info("è¯·ä¸Šä¼ ä¼ä¸šJSONæŠ¥å‘Šæˆ–é€‰æ‹©çŸ¥è¯†åº“æ–‡ä»¶ï¼Œå¹¶è¾“å…¥åœ°åŒºåç§°ã€‚")


# ========== è¡Œä¸šæŠ¥å‘Šåˆ†æ Tab ==========
with tabs[2]:
    st.write("è¯·ä¸Šä¼ ä¸€ä¸ªè¡Œä¸šä¸‹å¤šä¸ªå…¬å¸çš„JSONæ ¼å¼æŠ¥å‘Šæ–‡ä»¶ï¼Œæˆ–ä»çŸ¥è¯†åº“å¯¼å…¥ã€‚ç³»ç»Ÿå°†ç”Ÿæˆè¡Œä¸šç»“æ„åˆ†æåŠæ™ºèƒ½æ‘˜è¦ã€‚")

    use_kb = st.checkbox("ä»çŸ¥è¯†åº“å¯¼å…¥ JSON æ–‡ä»¶", value=False, key="industry_kb")
    #knowledge_base_dir = r"d:\projects\tavily-company-research\knowledge_base\company_reports"
    knowledge_base_dir = project_root / "tavily-company-research" / "knowledge_base" / "company_reports"

    uploaded_jsons = []
    if use_kb:
        if os.path.exists(knowledge_base_dir):
            kb_files = [f for f in os.listdir(knowledge_base_dir) if f.endswith(".json")]

            filter_text = st.text_input("æŒ‰å…³é”®è¯ç­›é€‰çŸ¥è¯†åº“æ–‡ä»¶ï¼ˆå¯æ‰¹é‡é€‰ä¸­ï¼‰", value="", key="industry_kb_filter")
            if filter_text:
                matched_files = [f for f in kb_files if filter_text in f]
            else:
                matched_files = kb_files

            select_all = st.checkbox("å…¨é€‰çŸ¥è¯†åº“æ–‡ä»¶", value=False, key="industry_kb_all")
            default_selection = matched_files if select_all else []

            selected_files = st.multiselect(
                "é€‰æ‹©çŸ¥è¯†åº“æ–‡ä»¶",
                options=kb_files,
                default=default_selection,
                key="industry_kb_files"
            )

            for f in selected_files:
                file_path = os.path.join(knowledge_base_dir, f)
                class FileLike:
                    def __init__(self, path):
                        self.name = os.path.basename(path)
                        self._path = path
                    def getvalue(self):
                        with open(self._path, "rb") as _f:
                            return _f.read()
                uploaded_jsons.append(FileLike(file_path))
        else:
            st.warning(f"çŸ¥è¯†åº“æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {knowledge_base_dir}")
    else:
        uploaded_jsons = st.file_uploader(
            "ä¸Šä¼ è¡Œä¸šå…¬å¸æŠ¥å‘Šï¼ˆæ”¯æŒå¤šæ–‡ä»¶ä¸Šä¼ ï¼‰", type=["json"], accept_multiple_files=True, key="industry_jsons"
        )

    industry_name = st.text_input("è¯·è¾“å…¥è¡Œä¸šåç§°ï¼ˆå¦‚ï¼šè¯åˆ¸ã€æ–°èƒ½æºã€äººå·¥æ™ºèƒ½ç­‰ï¼‰", value="è¯åˆ¸")

    if uploaded_jsons and industry_name:
        with tempfile.TemporaryDirectory() as tmp_dir:
            file_paths = []
            for file in uploaded_jsons:
                safe_name = file.name.replace("ï¼Œ","_").replace(" ", "_")
                file_path = os.path.join(tmp_dir, safe_name)
                with open(file_path, "wb") as f:
                    f.write(file.getvalue())
                file_paths.append(file_path)

            st.write(f"å·²ä¸Šä¼  {len(file_paths)} ä¸ªè¡Œä¸šæŠ¥å‘Šæ–‡ä»¶")
            output_json_path = os.path.join(tmp_dir, f"{industry_name}_industry_report.json")

            with st.spinner("æ­£åœ¨ç”Ÿæˆè¡Œä¸šåˆ†ææŠ¥å‘Šï¼Œè¯·ç¨å€™..."):
                agent = IndustryAgent()
                report = agent.analyze_industry(
                    industry_dir=tmp_dir,
                    industry_name=industry_name,
                    output_path=output_json_path
                )

            if report:
                st.success("è¡Œä¸šåˆ†æå®Œæˆï¼")
                st.markdown(f"ğŸ“ æŠ¥å‘Šå·²ä¿å­˜è‡³ï¼š`{output_json_path}`")

                st.subheader("è¡Œä¸šç»“æ„æ‘˜è¦")
                st.markdown(f"**è¡Œä¸šåç§°ï¼š** {report.get('industry_name', 'æœªçŸ¥')}")
                st.markdown(f"**å…¬å¸æ•°é‡ï¼š** {report.get('companies_count', 0)}")

                if "llm_report" in report:
                    st.subheader("LLMæ™ºèƒ½åˆ†ææŠ¥å‘Š")
                    st.markdown(report["llm_report"])

                txt_path = output_json_path.replace(".json", ".txt")
                if os.path.exists(txt_path):
                    with open(txt_path, "r", encoding="utf-8") as f:
                        txt_content = f.read()
                    st.subheader("åˆ†ææŠ¥å‘Šæ–‡æœ¬ï¼ˆå¯å¤åˆ¶/ä¸‹è½½ï¼‰")
                    st.text_area("è¡Œä¸šåˆ†ææŠ¥å‘Š", txt_content, height=400)
                    st.download_button("ä¸‹è½½txtæŠ¥å‘Š", txt_content, file_name=os.path.basename(txt_path))
            else:
                st.error("æœªç”Ÿæˆåˆ†ææŠ¥å‘Šï¼Œè¯·æ£€æŸ¥ä¸Šä¼ çš„JSONæ–‡ä»¶æˆ–çŸ¥è¯†åº“æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®ã€‚")
    else:
        st.info("è¯·ä¸Šä¼ è¡Œä¸šæŠ¥å‘Šæˆ–é€‰æ‹©çŸ¥è¯†åº“æ–‡ä»¶ï¼Œå¹¶è¾“å…¥è¡Œä¸šåç§°ã€‚")

# ========== ä¼ä¸šè´¢åŠ¡åˆ†æ Tab ==========
with tabs[3]:
    st.write("è¾“å…¥ä¼ä¸šåç§°ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨è·å–è´¢åŠ¡æ•°æ®å¹¶ç”Ÿæˆä¸“ä¸šåˆ†ææŠ¥å‘Š")
    
    # ä¼ä¸šåç§°è¾“å…¥
    company_name = st.text_input("è¯·è¾“å…¥ä¼ä¸šåç§°ï¼ˆå¦‚ï¼šApple Inc.ï¼‰", value="Apple Inc.")
    
    # é«˜çº§é€‰é¡¹
    with st.expander("é«˜çº§é€‰é¡¹"):
        max_results = st.slider("æœ€å¤§æœç´¢ç»“æœæ•°", 1, 10, 5)
        search_domains = st.multiselect(
            "é™å®šæœç´¢åŸŸå",
            options=["sec.gov", "investor.*", "finance.*", "nasdaq.com", "bloomberg.com"],
            default=["sec.gov", "investor.*", "finance.*"]
        )
    
    if st.button("ç”Ÿæˆè´¢åŠ¡åˆ†ææŠ¥å‘Š", key="financial_analysis"):
        if company_name:
            with st.spinner("æ­£åœ¨è·å–è´¢åŠ¡æ•°æ®å¹¶ç”Ÿæˆåˆ†ææŠ¥å‘Š..."):
                try:
                    # åˆå§‹åŒ–FinancialAgent
                    agent = FinancialAgent()
                    
                    # ç¬¬ä¸€æ­¥ï¼šè·å–è´¢åŠ¡æ•°æ®
                    financial_data = agent.tavily_search(
                        company_name=company_name
                    )
                    
                    # æ˜¾ç¤ºè·å–çš„è´¢åŠ¡æ•°æ®æ‘˜è¦
                    st.subheader("è·å–çš„è´¢åŠ¡æ•°æ®æ‘˜è¦")
                    st.json(financial_data)
                    
                    # ç¬¬äºŒæ­¥ï¼šç”Ÿæˆåˆ†ææŠ¥å‘Š
                    analysis_report = agent.deepseek_financial_analysis(
                        company=company_name,
                        financial_data=financial_data
                    )
                    
                    # æ˜¾ç¤ºåˆ†ææŠ¥å‘Š
                    st.success("è´¢åŠ¡åˆ†æå®Œæˆï¼")
                    st.subheader(f"{company_name} è´¢åŠ¡åˆ†ææŠ¥å‘Š")
                    st.markdown(analysis_report)
                    
                    # æä¾›ä¸‹è½½åŠŸèƒ½
                    report_filename = f"{company_name.replace(' ', '_')}_financial_report.txt"
                    st.download_button(
                        label="ä¸‹è½½è´¢åŠ¡åˆ†ææŠ¥å‘Š",
                        data=analysis_report,
                        file_name=report_filename,
                        mime="text/plain"
                    )
                    
                except Exception as e:
                    st.error(f"è´¢åŠ¡åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        else:
            st.warning("è¯·è¾“å…¥ä¼ä¸šåç§°")

#streamlit run web_app.py
# æˆ‘ç°åœ¨æœ‰ä¸¤ä¸ªé¡¹ç›®ï¼ˆA Bï¼‰éœ€è¦åˆå¹¶ä¸ºä¸€ä¸ªé¡¹ç›®ï¼Œå…¶ä¸­é¡¹ç›®Aç”Ÿæˆæ–‡ä»¶ä¸º.jsonæ ¼å¼çš„å…¬å¸æŠ¥å‘Šï¼ˆå¤šä¸ªï¼‰ï¼Œé¡¹ç›®Båˆ™è´Ÿè´£å°†æŠ¥å‘ŠæŠ•å–‚ç»™LLMè¿›è¡Œè¡Œä¸šçš„åˆ†æï¼›æˆ‘çš„éœ€æ±‚æ˜¯å½“Aå®Œæˆæ—¶èƒ½å¤Ÿæä¾›ä¸€ä¸ªè·³è½¬çš„é“¾æ¥é€šå¾€é¡¹ç›®Bï¼›ä»¥ä¸‹æ˜¯é¡¹ç›®Açš„ä»£ç ï¼š
